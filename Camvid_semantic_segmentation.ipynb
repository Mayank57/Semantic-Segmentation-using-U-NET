{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Camvid semantic segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXH0pkBCEQUc",
        "colab_type": "text"
      },
      "source": [
        "# Semantic Segmentation\n",
        "## What is Semantic Segmentation?\n",
        "Semantic segmentation refers to the process of linking each pixel in an image to a class label. It can also be referred to as **image classification for pixels** in an image. Its primary applications include autonomous vehicles, human-computer interaction, robotics and photo-editing tools. It is very useful for delf-driving cars in which contextual information of the environment is required at each and every step while traversing the route.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1000/1*wbaUQkYzRhvmd7IjKJjjCg.gif)\n",
        "\n",
        "\n",
        "## Aim of this project\n",
        "What we aim to do in this project is to perform semantic segmentation on CAMVID dataset and evaluate the deep learning model by using some metrics.\n",
        "\n",
        "## Deep Learning architecture\n",
        "For this project, I am going to use unet architecture, which was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. The architecture of this model is similar to that of an autoencoder. The encoding path consists of a series of convolutions, also known as the down-sampling path. The decoding path consists of upconvolutions (transposed convolutions are used in this case). It's also called the up-sampling path. The architecture of unet is fully convolutional, wherein we are using the Cross Entropy Loss.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1600/1*OkUrpDD6I0FpugA_bbYBJQ.png)\n",
        "\n",
        "## Specific details of the dataset used\n",
        "The Cambridge-driving Labeled Video Database (CamVid) is the first collection of videos with object class semantic labels, complete with metadata. The database provides ground truth labels that associate each pixel with one of 32 semantic classes.\n",
        "\n",
        "## Deep Learning Framework used\n",
        "I=We have used PyTorch as the deep learning framework for this task.\n",
        "\n",
        "## Steps involved\n",
        "\n",
        "\n",
        "1.   Data-preprocessing\n",
        "2.   Data-loading (using custom dataloaders)\n",
        "3.   Developing the deep learning architecture\n",
        "4.   Training the model\n",
        "5.   Evaluating the model:\n",
        "     1.   Pixel Accuracy\n",
        "     2.   Intersection over Union\n",
        "6.   Testing for a particular sample image\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ZiJZPfIpS_",
        "colab_type": "text"
      },
      "source": [
        "# Data-preprocessing\n",
        "In any machine learning task, most of the effort goes in converting the given raw data into the format we require. So, our main aim is to load the CAMVID dataset images, convert the segmented dataset into required ground truth label and storing this data into a list which shall be later fed to the dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQKNSAeyN0Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will import all the important libraries which are required for the execution of this task\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqz_r9wqO6yB",
        "colab_type": "code",
        "outputId": "5a6bf771-29c1-4e44-8dac-b1348c22bf2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# We will mount google drive here so that we can access the folders and files in google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive');"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDxpUxsLPCWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import train, validation and test images from the folders and store these in a list\n",
        "\n",
        "def load_img(folder):\n",
        "    images = [];\n",
        "    for filename in os.listdir(folder): # List all the filenames in the folder\n",
        "        img = cv2.imread(os.path.join(folder, filename)); # Join the link of the folder and filename\n",
        "        img = cv2.resize(img, (128, 128), interpolation = cv2.INTER_AREA);\n",
        "        images.append(img);\n",
        "    return images\n",
        "\n",
        "# To load labeled gray-scaled images\n",
        "\n",
        "def load_label_img(folder):\n",
        "    images = [];\n",
        "    for filename in os.listdir(folder): # List all the filenames in the folder\n",
        "        img = cv2.imread(os.path.join(folder, filename)); # Join the link of the folder and filename\n",
        "        img = cv2.resize(img, (128, 128), interpolation = cv2.INTER_AREA);\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY);\n",
        "        images.append(img);\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr2JAzuXKBQW",
        "colab_type": "text"
      },
      "source": [
        "We are converting the segmented label images into grayscale because it simplifies the problem of converting these images into ground truth label matrices (we shall see this later on)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLfYSFTdPg0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the output labels to pixel-wise classes\n",
        "\n",
        "def c2g(cn):\n",
        "    cn = np.reshape(cn, (1, 1, 3));\n",
        "    cn = cv2.cvtColor(cn, cv2.COLOR_BGR2GRAY);\n",
        "    return cn;\n",
        "\n",
        "# Pixels corresponding to each class are stored in the list in gray scale form\n",
        "\n",
        "colors = [];\n",
        "colors.append(c2g(np.array([64, 128, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 0, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 128, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 0, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 0, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 128, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 192, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 64, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 0, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 64, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 192, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 64, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 64, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 0, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 128, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 0, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 64, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([128, 64, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 128, 128], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([192, 128, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([64, 0, 64], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 192, 192], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 0, 0], dtype = 'uint8')));\n",
        "colors.append(c2g(np.array([0, 192, 64], dtype = 'uint8')));\n",
        "\n",
        "def class_pixel(label_img):\n",
        "    class_pix = np.ones([128, 128, 1], dtype = int);\n",
        "    for index, c in enumerate(colors):\n",
        "        class_pix[label_img == c] = index; # Vectorized masking is much much faster\n",
        "    return class_pix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Ba0XZpKjMi",
        "colab_type": "text"
      },
      "source": [
        "Our main aim in the above function is to convert segmented label images to ground truth label form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0sMsuDBPmNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all segmented images into labeled images\n",
        "\n",
        "def label_img_list(img_list):\n",
        "    images = [];\n",
        "    for image in img_list:\n",
        "        images.append(class_pixel(image));\n",
        "    return images;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUjLbHCkLIk3",
        "colab_type": "text"
      },
      "source": [
        "# Data-loading\n",
        "In this step, we define transforms that have be applied on the image as well as the image labels. Then we create custom dataset classes in order to load the train, validation and test images and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHRJG3PHPrAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the transformations that have to be applied on the images\n",
        "\n",
        "transform_img = transforms.Compose([ \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],   # input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "                        std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "transform_img_label = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# We have to create custom classes in order to use the DataLoader. These classes inherit the Dataset class\n",
        "\n",
        "class trainset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_train = None, root_train_label = None, transform_label = None):\n",
        "        self.train_img = load_img(root_train);\n",
        "        self.transform = transform;\n",
        "        self.transform_label = transform_label;\n",
        "        self.train_label_img = label_img_list(load_label_img(root_train_label));\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.train_img);\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.train_img[index]);\n",
        "        label = self.transform_label(self.train_label_img[index]);\n",
        "        return img, label;\n",
        "\n",
        "class valset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_val = None, root_val_label = None, transform_label = None):\n",
        "        self.val_img = load_img(root_val);\n",
        "        self.transform = transform;\n",
        "        self.transform_label = transform_label;\n",
        "        self.val_label_img = label_img_list(load_label_img(root_val_label));\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.val_img);\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.val_img[index]);\n",
        "        label = self.transform_label(self.val_label_img[index]);\n",
        "        return img, label\n",
        "\n",
        "    \n",
        "class testset(data.Dataset):\n",
        "    def __init__(self, transform = None, root_test = None, root_test_label = None, transform_label = None):\n",
        "        self.test_img = load_img(root_test);\n",
        "        self.transform = transform;\n",
        "        self.transform_label = transform_label;\n",
        "        self.test_label_img = label_img_list(load_label_img(root_test_label));\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.test_img);\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.transform(self.test_img[index]);\n",
        "        label = self.transform_label(self.test_label_img[index]);\n",
        "        return img, label\n",
        "\n",
        "\n",
        "traindataset = trainset(transform_img, '/content/gdrive/My Drive/Camvid dataset/train/', '/content/gdrive/My Drive/Camvid dataset/train_labels/', transform_img_label);\n",
        "valdataset = valset(transform_img, '/content/gdrive/My Drive/Camvid dataset/val/', '/content/gdrive/My Drive/Camvid dataset/val_labels/', transform_img_label);\n",
        "testdataset = testset(transform_img, '/content/gdrive/My Drive/Camvid dataset/test/', '/content/gdrive/My Drive/Camvid dataset/test_labels/', transform_img_label);\n",
        "\n",
        "# We have to instantiate these classes and feed this to the dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOl8sxAxIMa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders are used for loading the dataset\n",
        "\n",
        "train_loader = data.DataLoader(traindataset, batch_size = 2, shuffle=True,  num_workers=4);\n",
        "val_loader = data.DataLoader(valdataset, batch_size = 2, shuffle=True,  num_workers=4);\n",
        "test_loader = data.DataLoader(testdataset, batch_size = 2, shuffle=True,  num_workers=4);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_2i8QRKMW0u",
        "colab_type": "text"
      },
      "source": [
        "# Developing the deep learning architecture\n",
        "Here we created the unet architecture for a 128 by 128 input image size. We have also devised an architecture for a 512 by 512 input image size. In order to have less gpu memory usage, we chose the 128 by 128 image architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHi1BGUiIQIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class u_net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__();\n",
        "    self.conv1 = nn.Conv2d(3, 64, 3);\n",
        "    self.conv2 = nn.Conv2d(64, 128, 3);\n",
        "    self.conv3 = nn.Conv2d(128, 256, 3);\n",
        "    self.conv4 = nn.Conv2d(256, 512, 3);\n",
        "    self.conv5 = nn.Conv2d(512, 1024, 3);\n",
        "    self.conv6 = nn.Conv2d(1024, 512, 3);\n",
        "    self.conv7 = nn.Conv2d(512, 512, 3);\n",
        "    self.conv8 = nn.Conv2d(512, 256, 3);\n",
        "    self.conv9 = nn.Conv2d(256, 256, 3);\n",
        "    self.conv10 = nn.Conv2d(256, 128, 3);\n",
        "    self.conv11 = nn.Conv2d(128, 128, 3);\n",
        "    self.b1 = nn.BatchNorm2d(64);\n",
        "    self.b2 = nn.BatchNorm2d(128);\n",
        "    self.b3 = nn.BatchNorm2d(256);\n",
        "    self.b4 = nn.BatchNorm2d(512);\n",
        "    self.b5 = nn.BatchNorm2d(1024);\n",
        "    self.convT1 = nn.ConvTranspose2d(1024, 512, 2, 2);\n",
        "    self.convT2 = nn.ConvTranspose2d(512, 256, 2, 2);\n",
        "    self.convT3 = nn.ConvTranspose2d(256, 256, 2, 2);\n",
        "    self.convT4 = nn.ConvTranspose2d(128, 64, 2, 2);\n",
        "    self.convT5 = nn.ConvTranspose2d(64, 32, 2, 2);\n",
        "    self.pool1 = nn.MaxPool2d(2, 2);\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.b1(self.conv1(x)));\n",
        "    x = F.relu(self.b2(self.conv2(x)));\n",
        "    x = self.pool1(x);\n",
        "    x = F.relu(self.b2(self.conv11(x)));\n",
        "    x = F.relu(self.b3(self.conv3(x)));\n",
        "    x1 = x;\n",
        "    x1 = x1[:, :, int((58 - 24)/2) : int((58 + 24)/2), int((58 - 24)/2) : int((58 + 24)/2)];\n",
        "    x = self.pool1(x);\n",
        "    x = F.relu(self.b3(self.conv9(x)));\n",
        "    x = F.relu(self.b4(self.conv4(x)));\n",
        "    x2 = x;\n",
        "    x2 = x2[:, :, int((25 - 16)/2) : int((25 + 16)/2), int((25 - 16)/2) : int((25 + 16)/2)];\n",
        "    x = self.pool1(x);\n",
        "    x = F.relu(self.b4(self.conv7(x)));\n",
        "    x = F.relu(self.b5(self.conv5(x)));\n",
        "    x = self.b4(self.convT1(x));\n",
        "    x = torch.cat((x2, x), dim = 1);\n",
        "    x = F.relu(self.b4(self.conv6(x)));\n",
        "    x = F.relu(self.b4(self.conv7(x)));\n",
        "    x = self.b3(self.convT2(x));\n",
        "    x = torch.cat((x1, x), dim = 1);\n",
        "    x = F.relu(self.b3(self.conv8(x)));\n",
        "    x = F.relu(self.b3(self.conv9(x)));\n",
        "    x = self.b3(self.convT3(x));\n",
        "    x = F.relu(self.b2(self.conv10(x)));\n",
        "    x = F.relu(self.b2(self.conv11(x)));\n",
        "    x = F.relu(self.b2(self.conv11(x)));\n",
        "    x = F.relu(self.b2(self.conv11(x)));\n",
        "    x = self.b1(self.convT4(x));\n",
        "    x = self.convT5(x);\n",
        "    del x1;\n",
        "    del x2;\n",
        "    return x;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV3_ZwA-IU0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class u_net512(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__();\n",
        "    self.conv1 = nn.Conv2d(3, 64, 3);\n",
        "    self.b1 = nn.BatchNorm2d(64);\n",
        "    self.conv2 = nn.Conv2d(64, 64, 3);\n",
        "    self.pool1 = nn.MaxPool2d(2, 2);\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3);\n",
        "    self.b2 = nn.BatchNorm2d(128);\n",
        "    self.conv4 = nn.Conv2d(128, 128, 3);\n",
        "    self.conv5 = nn.Conv2d(128, 256, 3);\n",
        "    self.b3 = nn.BatchNorm2d(256)\n",
        "    self.conv6 = nn.Conv2d(256, 256, 3);\n",
        "    self.conv7 = nn.Conv2d(256, 512, 3);\n",
        "    self.b4 = nn.BatchNorm2d(512);\n",
        "    self.conv8 = nn.Conv2d(512, 512, 3);\n",
        "    self.conv9 = nn.Conv2d(512, 1024, 3);\n",
        "    self.b5 = nn.BatchNorm2d(1024);\n",
        "    self.conv10 = nn.Conv2d(1024, 1024, 3);\n",
        "    self.convT1 = nn.ConvTranspose2d(1024, 512, 2, 2);\n",
        "    self.conv11 = nn.Conv2d(1024, 512, 3);\n",
        "    self.conv12 = nn.Conv2d(512, 512, 3);\n",
        "    self.convT2 = nn.ConvTranspose2d(512, 256, 2, 2);\n",
        "    self.conv13 = nn.Conv2d(512, 256, 3);\n",
        "    self.conv14 = nn.Conv2d(256, 256, 3);\n",
        "    self.convT3 = nn.ConvTranspose2d(256, 128, 2, 2);\n",
        "    self.conv15 = nn.Conv2d(256, 128, 3);\n",
        "    self.conv16 = nn.Conv2d(128, 128, 3);\n",
        "    self.convT4 = nn.ConvTranspose2d(128, 64, 2, 2);\n",
        "    self.convT5 = nn.ConvTranspose2d(64, 32, 2, 2);\n",
        "    self.conv17 = nn.Conv2d(32, 32, 145);\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.b1(self.conv1(x)));\n",
        "    x = F.relu(self.b1(self.conv2(x)));\n",
        "    x = self.pool1(x);\n",
        "    x = F.relu(self.b2(self.conv3(x)));\n",
        "    x = F.relu(self.b2(self.conv4(x)));\n",
        "    x1 = x;\n",
        "    x1 = x1[:, :, int((250 - 168)/2) : int((250 + 168)/2), int((250 - 168)/2) : int((250 + 168)/2)];\n",
        "    x = self.pool1(x);\n",
        "    x = F.relu(self.b3(self.conv5(x)));\n",
        "    x = F.relu(self.b3(self.conv6(x)));\n",
        "    x2 = x;\n",
        "    x2 = x2[:, :, int((121 - 88)/2) : int((121 + 88)/2), int((121 - 88)/2) : int((121 + 88)/2)];\n",
        "    x = self.pool1(x);\n",
        "    x = F.relu(self.b4(self.conv7(x)));\n",
        "    x = F.relu(self.b4(self.conv8(x)));\n",
        "    x3 = x;\n",
        "    x3 = x3[:, :, int((56 - 48)/2) : int((56 + 48)/2), int((56 - 48)/2) : int((56 + 48)/2)];\n",
        "    x = self.pool1(x);\n",
        "    x = F.relu(self.b5(self.conv9(x)));\n",
        "    x = F.relu(self.b5(self.conv10(x)));\n",
        "    x = self.b4(self.convT1(x));\n",
        "    x = torch.cat((x3, x), dim = 1);\n",
        "    x = F.relu(self.b4(self.conv11(x)));\n",
        "    x = F.relu(self.b4(self.conv12(x)));\n",
        "    x = self.b3(self.convT2(x)); \n",
        "    x = torch.cat((x2, x), dim = 1);\n",
        "    x = F.relu(self.b3(self.conv13(x)));\n",
        "    x = F.relu(self.b3(self.conv14(x)));\n",
        "    x = self.b2(self.convT3(x)); \n",
        "    x = torch.cat((x1, x), dim = 1);\n",
        "    x = F.relu(self.b2(self.conv15(x)));\n",
        "    x = F.relu(self.b2(self.conv16(x)));\n",
        "    x = self.b1(self.convT4(x)); \n",
        "    x = self.convT5(x);\n",
        "    x = F.relu(self.conv17(x));\n",
        "    del x1;\n",
        "    del x2;\n",
        "    del x3;\n",
        "    return x;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAcRwE_PQJYq",
        "colab_type": "code",
        "outputId": "e1689df4-ab87-4d44-cf0e-cf7e006b288d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Putting the model into the gpu\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
        "print(device)\n",
        "UNET = u_net();\n",
        "UNET.to(device);"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDBnT5Z2QTwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the loss functions and the optimizers that we are going to use here\n",
        "\n",
        "criterion = nn.CrossEntropyLoss();\n",
        "optimizer = optim.Adam(UNET.parameters(), lr = 0.0001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgAOyT1PNmXx",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5tg5vqBQa3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = []\n",
        "\n",
        "for j in range(500):\n",
        "  for i, data in enumerate(train_loader):  \n",
        "    inputs, labels = data;\n",
        "    if labels.size() == torch.Size([1, 1, 128, 128]):\n",
        "      labels = labels.reshape(1, 128, 128);\n",
        "    elif labels.size() == torch.Size([2, 1, 128, 128]):\n",
        "      labels = labels.reshape(2, 128, 128);\n",
        "    inputs, labels = inputs.to(device), labels.to(device);\n",
        "    optimizer.zero_grad();\n",
        "    outputs = UNET(inputs);\n",
        "    loss = criterion(outputs, labels);\n",
        "    loss.backward();\n",
        "    optimizer.step();\n",
        "    losses.append(loss.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29nWNHk8N-Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualization of the loss (convergence)\n",
        "\n",
        "plt.plot(losses)\n",
        "print('The variation of loss function can be seen as:')\n",
        "plt.show()\n",
        "\n",
        "# We will use a gaussian filter to smoothen the curve\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "ls = gaussian_filter1d(losses, 100) # Where 1 is the standard deviation\n",
        "print('After applying Gaussian filter:')\n",
        "plt.plot(ls)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_axpSndQuHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model\n",
        "\n",
        "PATH = '/content/gdrive/My Drive/Camvid dataset/saved.pth';\n",
        "torch.save(UNET.state_dict(),PATH); # A state_dict is simply a Python dictionary \n",
        "# object that maps each layer to its parameter tensor. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJcDMKaMI-_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model\n",
        "\n",
        "PATH = '/content/gdrive/My Drive/Camvid dataset/saved.pth';\n",
        "UNET.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLx-IyA-QKWB",
        "colab_type": "text"
      },
      "source": [
        "# Metrics for evaluating the model\n",
        "## Pixel-wise accuarcy:\n",
        "\n",
        "It is the percentage of pixels in the image that are classified correctly. But the main issue in this kind of classification is in the case of **class-imbalance**. Suppose, we have an image wherein there are two classes. We have to perform semantic segmentation on this image. But the scenario is that - the first class covers 95 percent of the image. So while training, the model will very well learn about the presence of the class that covers most of the image. So the test accuracy can be very high. But the second class might not get detected correctly. We won't know about that if we use pixel-wise accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg_r0yuTfKzH",
        "colab_type": "code",
        "outputId": "3f209bd6-cfbe-4bdf-a07f-9238077fa2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Compute test-set accuracy\n",
        "\n",
        "def corr(x): # To get proper correspondence between the outputs and the labels\n",
        "    x = x.cpu();\n",
        "    x = x.detach().numpy(); # Detach() was used as one can't convert a pytorch tensor to a numpy array if\n",
        "    # required_grad is set True for that variable\n",
        "    x = x.argmax(axis = 1);\n",
        "    return x;\n",
        "\n",
        "correct = 0;\n",
        "total = 0;\n",
        "for i, data in enumerate(val_loader):\n",
        "    inputs, labels = data;\n",
        "    inputs = inputs.to(device);\n",
        "    outputs = UNET.forward(inputs);\n",
        "    outputs = corr(outputs);\n",
        "    if labels.size() == torch.Size([outputs.shape[0], 1, 128, 128]):\n",
        "      labels = labels.detach().numpy();\n",
        "      labels = labels.reshape(outputs.shape[0], 128, 128);\n",
        "    # labels = labels.reshape(4, 512, 512);\n",
        "    outputs = outputs.reshape(outputs.shape[0], 128, 128);\n",
        "    for i in range(128):\n",
        "      for j in range(128):\n",
        "        for k in range(1):\n",
        "          total = total + 1;\n",
        "          if outputs[k, i, j] == labels[k, i, j]:\n",
        "            correct = correct + 1;\n",
        "print(\"Hence, the pixel-wise accuracy is \", (correct/total) * 100);"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hence, the pixel-wise accuracy is  40.53442818777902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWlZBH5giWVm",
        "colab_type": "text"
      },
      "source": [
        "## Intersection Over Union (Jaccard Index)\n",
        "This is a better metric than pixel wise accuracy. This is because, here we calculate the intersection over union for every class and then take their average. So, if any class-imbalance is present, it will definitely get pointed out by this metric.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/450/0*kraYHnYpoJOhaMzq.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "catv4ZXtiUOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IOUf = 0\n",
        "count = 0\n",
        "for i, data in enumerate(test_loader):\n",
        "    inputs, labels = data;\n",
        "    inputs = inputs.to(device);\n",
        "    outputs = UNET.forward(inputs);\n",
        "    outputs = corr(outputs);\n",
        "    if labels.size() == torch.Size([outputs.shape[0], 1, 128, 128]):\n",
        "      labels = labels.detach().numpy();\n",
        "      labels = labels.reshape(outputs.shape[0], 128, 128);\n",
        "    outputs = outputs.reshape(outputs.shape[0], 128, 128);\n",
        "    for k in range(outputs.shape[0]):\n",
        "      o = outputs[k, :, :]\n",
        "      l = labels[k, :, :]\n",
        "      IOU = 0\n",
        "      for c in range(32):\n",
        "        intersectionc = 0\n",
        "        unionc = 0\n",
        "        f = c * np.ones((1, 128, 128))\n",
        "        i = (o == c) * 1\n",
        "        j = (l == c) * 1\n",
        "        inte = 0\n",
        "        for x in range(128):\n",
        "          for y in range(128):\n",
        "            if i[x, y] == j[x, y] and i[x, y] == 1:\n",
        "              inte = inte + 1\n",
        "        intersectionc = intersectionc + inte\n",
        "        unionc = unionc + ((f == l) * 1).sum() + ((f == o) * 1).sum() - inte\n",
        "        if unionc == 0:\n",
        "         continue\n",
        "        IOUc = (intersectionc)/(unionc)\n",
        "        IOU = IOU + IOUc\n",
        "      count = count + 1\n",
        "      IOU = IOU/32\n",
        "    IOUf = IOUf + IOU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xeSXmoMs4jZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78145437-43b4-428f-944d-ba928ace4d18"
      },
      "source": [
        "print('The intersection-over-union is', IOUf/count * 100)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The intersection over union is 17.42041493261219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxTXoTKZ7DpO",
        "colab_type": "text"
      },
      "source": [
        "# Testing the model for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-02MIDT0fN4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting back to colored form\n",
        "\n",
        "# Pixels corresponding to each class are stored in the list\n",
        "colors = [];\n",
        "colors.append(np.array([64, 128, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 0, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 128, 0], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 128, 0], dtype = 'uint8'));\n",
        "colors.append(np.array([0, 0, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 0, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 0, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 128, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 192, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 64, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 0, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 0, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 128, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 0, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 64, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 192, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([0, 64, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 64, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 128, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 0, 0], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 128, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 128, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 128, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 0, 0], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 64, 0], dtype = 'uint8'));\n",
        "colors.append(np.array([128, 64, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([0, 128, 128], dtype = 'uint8'));\n",
        "colors.append(np.array([192, 128, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([64, 0, 64], dtype = 'uint8'));\n",
        "colors.append(np.array([0, 192, 192], dtype = 'uint8'));\n",
        "colors.append(np.array([0, 0, 0], dtype = 'uint8'))\n",
        "colors.append(np.array([0, 192, 64], dtype = 'uint8'));\n",
        "\n",
        "def test(op_img):\n",
        "    class_pix = np.ones([128, 128, 3], dtype = 'uint8');\n",
        "    for index, c in enumerate(colors):\n",
        "        class_pix[op_img == index] = c; # Vectorized masking is much much faster\n",
        "    return class_pix.reshape((128, 128, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QVbtnxZ93JH",
        "colab_type": "code",
        "outputId": "d855369b-1301-4cb3-f822-e1b6d7714594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "source": [
        "a = test(outputs[0]);\n",
        "a = a.reshape([128, 128, 3])\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "a = cv2.resize(a, (512, 512), interpolation = cv2.INTER_AREA)\n",
        "cv2_imshow(a)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAIACAIAAAB7GkOtAAAUnUlEQVR4nO3dUbKbOrMG0J1bGRhD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\nAABKAgBAKAEAINT/AK9VXgcCMetQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=512x512 at 0x7FF2D822DCF8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}